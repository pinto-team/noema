# NOEMA • config/value.yaml
# Reward/Value weights for the controller (used by value/reward.py).

# External reward weight (coach/environment: r_ext ∈ [-1, +1])
w_ext: 1.0

# Intrinsic reward weight (learning progress / curiosity)
w_int: 0.6

# Penalties
lambda_risk: 0.5    # weight for risk penalty
mu_energy: 0.05     # weight for energy/compute penalty

# Mild shaping (stability)
conf_bonus: 0.05    # small bonus for calibrated confidence
u_penalty: 0.05     # small penalty for higher uncertainty

# Final clipping of shaped reward
clip_min: -1.0
clip_max: 1.0
